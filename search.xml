<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>随便写写</title>
      <link href="/posts/380483a.html"/>
      <url>/posts/380483a.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>LLEMMA基于数学的问答语言模型学习记录</title>
      <link href="/posts/bdb6f40d.html"/>
      <url>/posts/bdb6f40d.html</url>
      
        <content type="html"><![CDATA[<h1>1.Introduction</h1><div class="note info flat"><p>参考：<a href="https://blog.eleuther.ai/llemma/">Llemma：开放的数学语言模型</a></p></div><h2 id="1-1-LIemma模型介绍">1.1 LIemma模型介绍</h2><p>如图所示，Llemma 模型使用 Code Llama 权重进行初始化，然后在 Proof-Pile II（一个包含 550 亿个数学和科学文档的令牌数据集）上进行训练。由此产生的模型显示出改进的数学能力，并且可以通过提示或额外的微调来适应各种任务 `<img src="https://blog.eleuther.ai/images/blog/llemma/llemma_diagram.jpeg" alt="L1emma模型流程图"></p><h2 id="1-2-比较有意思的训练方法，也是语言模型微调常用方法">1.2 比较有意思的训练方法，也是语言模型微调常用方法</h2><p>这段代码是在训练一个基于LLM（语言模型）的文本生成模型。具体来说，它执行以下操作：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">1.</span> 解析命令行参数，包括模型、数据和训练参数。</span><br><span class="line"><span class="bullet">2.</span> 从预训练模型中加载模型架构。</span><br><span class="line"><span class="bullet">3.</span> 使用指定的分词器对输入数据进行编码。</span><br><span class="line"><span class="bullet">4.</span> 如果分词器没有填充标记，则添加默认的填充标记。</span><br><span class="line"><span class="bullet">5.</span> 如果模型名称包含&quot;llama&quot;，则为分词器添加特殊的结束符、开始符和未知标记。</span><br><span class="line"><span class="bullet">6.</span> 使用分词器和数据参数创建一个监督式数据模块。</span><br><span class="line"><span class="bullet">7.</span> 使用模型、分词器、训练参数和数据模块创建一个训练器对象。</span><br><span class="line"><span class="bullet">8.</span> 使用训练器对象进行训练。</span><br><span class="line"><span class="bullet">9.</span> 保存训练器的当前状态。</span><br><span class="line"><span class="bullet">10.</span> 将训练好的模型保存到指定的输出目录。</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))</span><br><span class="line">    model_args, data_args, training_args, remaining_args = parser.parse_args_into_dataclasses(return_remaining_strings=<span class="literal">True</span>)</span><br><span class="line">    data_args.data_length = <span class="built_in">int</span>(remaining_args[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    model = transformers.AutoModelForCausalLM.from_pretrained(</span><br><span class="line">        model_args.model_name_or_path,</span><br><span class="line">        cache_dir=training_args.cache_dir,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    tokenizer = transformers.AutoTokenizer.from_pretrained(</span><br><span class="line">        <span class="string">&quot;hf-internal-testing/llama-tokenizer&quot;</span>,</span><br><span class="line">        cache_dir=training_args.cache_dir,</span><br><span class="line">        model_max_length=training_args.model_max_length,</span><br><span class="line">        padding_side=<span class="string">&quot;right&quot;</span>,</span><br><span class="line">        use_fast=<span class="literal">False</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">if</span> tokenizer.pad_token <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        smart_tokenizer_and_embedding_resize(</span><br><span class="line">            special_tokens_dict=<span class="built_in">dict</span>(pad_token=DEFAULT_PAD_TOKEN),</span><br><span class="line">            tokenizer=tokenizer,</span><br><span class="line">            model=model,</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;llama&quot;</span> <span class="keyword">in</span> model_args.model_name_or_path:</span><br><span class="line">        tokenizer.add_special_tokens(</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;eos_token&quot;</span>: DEFAULT_EOS_TOKEN,</span><br><span class="line">                <span class="string">&quot;bos_token&quot;</span>: DEFAULT_BOS_TOKEN,</span><br><span class="line">                <span class="string">&quot;unk_token&quot;</span>: DEFAULT_UNK_TOKEN,</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)</span><br><span class="line">    trainer = Trainer(model=model, tokenizer=tokenizer, args=training_args, **data_module)</span><br><span class="line">    trainer.train()</span><br><span class="line">    trainer.save_state()</span><br><span class="line">    <span class="comment"># if os.environ.get(&#x27;LOCAL_RANK&#x27;) == &#x27;0&#x27;:</span></span><br><span class="line">    safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)</span><br></pre></td></tr></table></figure><h2 id="1-3-论文贪心搜索的源码的阅读以及理解">1.3 论文贪心搜索的源码的阅读以及理解</h2><p>文章使用了贪心搜索的策略对于数据进行一定处理，代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">best_first_search</span>(<span class="params"></span></span><br><span class="line"><span class="params">        theorem,</span></span><br><span class="line"><span class="params">        model,</span></span><br><span class="line"><span class="params">        tokenizer,</span></span><br><span class="line"><span class="params">        max_iters,</span></span><br><span class="line"><span class="params">        temperatures,</span></span><br><span class="line"><span class="params">        num_samples,</span></span><br><span class="line"><span class="params">        prompt_fn,</span></span><br><span class="line"><span class="params">        timeout=<span class="number">600</span>,</span></span><br><span class="line"><span class="params">        early_stop=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        max_tokens=<span class="number">256</span></span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="built_in">dict</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Best first search.&quot;&quot;&quot;</span></span><br><span class="line">    attempt_results = []</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">with</span> Dojo(theorem, hard_timeout=timeout) <span class="keyword">as</span> (dojo, init_state):</span><br><span class="line">            start = time.time()</span><br><span class="line">            proof_finished = <span class="literal">False</span></span><br><span class="line">            queue = [(<span class="number">0.0</span>, [], init_state, [])]</span><br><span class="line">            visited = <span class="built_in">set</span>()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> iteration <span class="keyword">in</span> trange(max_iters):</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(queue) == <span class="number">0</span> <span class="keyword">or</span> proof_finished:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">                total_score, steps, state, trace = heapq.heappop(queue)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这段代码是一个名为<code>best_first_search</code>的函数，它实现了一种称为&quot;最佳优先搜索&quot;（Best First Search）的算法。这个算法用于在给定的定理和模型中寻找证明的最佳路径。<br>函数接受以下参数：</p><ul><li><code>theorem</code>：需要证明的定理对象。</li><li><code>model</code>：用于生成证明的语言模型。</li><li><code>tokenizer</code>：用于将文本转换为模型可以理解的输入格式的分词器。</li><li><code>max_iters</code>：最大迭代次数，即搜索的最大深度。</li><li><code>temperatures</code>：一个温度值列表，用于控制语言模型生成的概率分布。</li><li><code>num_samples</code>：每个步骤中从语言模型中采样的次数。</li><li><code>prompt_fn</code>：一个函数，根据当前的状态生成提示信息。</li><li><code>timeout</code>：搜索超时时间，单位为秒。</li><li><code>early_stop</code>：是否在找到第一个有效证明后提前停止搜索。</li><li><code>max_tokens</code>：生成的提示信息的最大长度。</li></ul><p>函数返回一个字典列表，每个字典包含以下键值对：</p><ul><li><code>'theorem'</code>：定理的全名。</li><li><code>'proof'</code>：证明的步骤序列。</li><li><code>'score'</code>：证明的得分，表示该证明的质量。</li><li><code>'success'</code>：证明是否成功。</li><li><code>'failure_reason'</code>：证明失败的原因。</li><li><code>'trace'</code>：证明过程中的轨迹信息。</li><li><code>'temperature'</code>：使用的模型温度。</li><li><code>'elapsed'</code>：搜索所花费的时间。</li><li><code>'iteration'</code>：当前的迭代次数。</li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> 语言模型 </tag>
            
            <tag> L1mma </tag>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
